{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abisubramanya27/CS6910_Assignment3/blob/master/src/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anLHOjMmcDNp",
        "outputId": "12fbe2b7-37c3-459c-f1de-70255205ce8e"
      },
      "source": [
        "!wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-25 07:53:50--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 74.125.20.128, 108.177.98.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   104MB/s    in 13s     \n",
            "\n",
            "2021-04-25 07:54:03 (151 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8z7gFejcpWM"
      },
      "source": [
        "!tar xopf dakshina_dataset_v1.0.tar"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Ixguaue_fq",
        "outputId": "064630bc-cb6d-47b2-e3b3-c1ddf0000d47"
      },
      "source": [
        "!ls dakshina_dataset_v1.0/hi/lexicons"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi.translit.sampled.dev.tsv   hi.translit.sampled.train.tsv\n",
            "hi.translit.sampled.test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-xkKp8jY8Q9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_data(data_path, characters = False):\n",
        "    # Returns the (input, output) pair from the dataset\n",
        "    # If characters == True, the input/output would be in the form list of characters, else as string\n",
        "\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [line.split(\"\\t\") for line in f.read().split(\"\\n\") if line != '']\n",
        "    \n",
        "    input, target = [val[1] for val in lines], [val[0] for val in lines]\n",
        "    if characters:\n",
        "        input, target = [list(inp_str) for inp_str in input], [list(tar_str) for tar_str in target]\n",
        "    return input, target\n",
        "\n",
        "\n",
        "def process_data(input, output, enc_timesteps, dec_timesteps, input_char_enc, target_char_enc):\n",
        "    # Returns the input and output data in a form needed by the Keras embedding layer, where each character is encoded by an integer\n",
        "\n",
        "    # ' ' -- space (equivalent to no meaningful input)\n",
        "    encoder_input = np.array([[input_char_enc[ch] for ch in string] + [input_char_enc[' ']] * (enc_timesteps - len(string)) for string in input])\n",
        "    # '\\t' -- start of sequence, '\\n' -- end of sequence\n",
        "    decoder_input = np.array([[target_char_enc['\\t']] + [target_char_enc[ch] for ch in string] + [target_char_enc['\\n']] \n",
        "                                 + [target_char_enc[' ']] * (dec_timesteps - len(string) - 2) for string in output])\n",
        "    decoder_output = np.zeros((decoder_input.shape[0], dec_timesteps, len(target_char_enc)), dtype='float32')\n",
        "\n",
        "    for i in range(decoder_input.shape[0]):\n",
        "        for t, char_ind in enumerate(decoder_input[i]):\n",
        "            if t > 0:\n",
        "                decoder_output[i,t-1,char_ind] = 1.0\n",
        "        decoder_output[i,t:,target_char_enc[' ']] = 1.0\n",
        "\n",
        "    return encoder_input, decoder_input, decoder_output\n",
        "\n",
        "\n",
        "def encode_decode_characters(train_input, train_target, valid_input, valid_target):\n",
        "    # Returns the encoder for characters to integer (as a dictionary) and decoder for integers to characters (as a list) for input and target data\n",
        "\n",
        "    input_char_enc = {}\n",
        "    input_char_dec = []\n",
        "    max_encoder_seq_length = 1\n",
        "    for string in train_input + valid_input:\n",
        "        max_encoder_seq_length = max(max_encoder_seq_length, len(string))\n",
        "        for char in string:\n",
        "            if char not in input_char_enc:\n",
        "                input_char_enc[char] = len(input_char_dec)\n",
        "                input_char_dec.append(char)\n",
        "    input_char_enc[' '] = len(input_char_dec)\n",
        "    input_char_dec.append(' ')\n",
        "\n",
        "    target_char_enc = {}\n",
        "    target_char_dec = []\n",
        "    target_char_enc['\\t'] = len(target_char_dec)\n",
        "    target_char_dec.append('\\t')\n",
        "    max_decoder_seq_length = 1\n",
        "    for string in train_target + valid_target:\n",
        "        max_decoder_seq_length = max(max_decoder_seq_length, len(string)+2)\n",
        "        for char in string:\n",
        "            if char not in target_char_enc:\n",
        "                target_char_enc[char] = len(target_char_dec)\n",
        "                target_char_dec.append(char)\n",
        "    target_char_enc['\\n'] = len(target_char_dec)\n",
        "    target_char_dec.append('\\n')\n",
        "    target_char_enc[' '] = len(target_char_dec)\n",
        "    target_char_dec.append(' ')\n",
        "\n",
        "    print(\"Number of training samples:\", len(train_input))\n",
        "    print(\"Number of validation samples:\", len(valid_input))\n",
        "    print(\"Number of unique input tokens:\", len(input_char_dec))\n",
        "    print(\"Number of unique output tokens:\", len(target_char_dec))\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    return input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97ou-pxE9gCQ",
        "outputId": "bc03a0ec-c503-4166-8e8a-8e882d60afc7"
      },
      "source": [
        "input_char_enc = {}\n",
        "input_char_dec = []\n",
        "target_char_enc = {}\n",
        "target_char_dec = []\n",
        "max_encoder_seq_length = 0\n",
        "max_decoder_seq_length = 0\n",
        "\n",
        "# Reading training and validation data\n",
        "train_inp, train_out = read_data('./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv', True)\n",
        "valid_inp, valid_out = read_data('./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv', True)\n",
        "# Assigning encoder and decoder for input and target characters\n",
        "input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length = encode_decode_characters(train_inp, train_out, valid_inp, valid_out)\n",
        "\n",
        "# Assigning training and validation encoder input, decoder input, decoder output\n",
        "train_enc_input, train_dec_input, train_dec_output = process_data(train_inp, train_out, max_encoder_seq_length, max_decoder_seq_length, \n",
        "                                                                  input_char_enc, target_char_enc)\n",
        "valid_enc_input, valid_dec_input, valid_dec_output = process_data(valid_inp, valid_out, max_decoder_seq_length, max_decoder_seq_length, \n",
        "                                                                  input_char_enc, target_char_enc)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 44204\n",
            "Number of validation samples: 4358\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8jvETu5Ga6g"
      },
      "source": [
        "def get_model(inp_emb_size=16):\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClA5ym6tw2fp",
        "outputId": "e77c357b-56ad-45bc-cb7e-d1840f323165"
      },
      "source": [
        "a = 'abc'\n",
        "print(len(a))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}